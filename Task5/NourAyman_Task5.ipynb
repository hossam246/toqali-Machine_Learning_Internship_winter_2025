{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import randint\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load your cleaned dataset\n",
    "df = pd.read_csv(\"processed_data.csv\")\n",
    "\n",
    "# Drop the original datetime column\n",
    "df.drop(columns=['pickup_datetime'], inplace=True)\n",
    "df.drop(columns=['key'], inplace=True)\n",
    "# Drop any rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=['fare_amount'])  # Features\n",
    "y = df['fare_amount']  # Target\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_features = ['Weather', 'Traffic Condition', 'Car Condition']\n",
    "numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# # Preprocessing: One-Hot Encoding for categorical & Scaling for numerical\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', StandardScaler(), numerical_features),\n",
    "#     ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "# ])\n",
    "# Reduce One-Hot Encoding memory issue by using Ordinal Encoding\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OrdinalEncoder(), categorical_features)\n",
    "])\n",
    "\n",
    "# Split the dataset (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your dataset\n",
    "# df = pd.read_csv(\"final_internship_data.csv\")\n",
    "\n",
    "# # Drop the original datetime column\n",
    "# df.drop(columns=['User ID', 'User Name', 'Driver Name', 'pickup_datetime'], inplace=True)\n",
    "# df.drop(columns=['key'], inplace=True)\n",
    "# # Drop any rows with missing values\n",
    "# df.dropna(inplace=True)\n",
    "\n",
    "# for column in df.select_dtypes('float64', 'int64'):\n",
    "#     q1 = df[column].quantile(0.25)\n",
    "#     q3 = df[column].quantile(0.75)\n",
    "#     iqr = q3 - q1\n",
    "#     lower_bound = q1 - 1.5 * iqr\n",
    "#     upper_bound = q3 + 1.5 * iqr\n",
    "#     df[column] = df[column].apply(lambda x: min(upper_bound, max(lower_bound, x)))\n",
    "\n",
    "# # Define features and target\n",
    "# X = df.drop(columns=['fare_amount'])  # Features\n",
    "# y = df['fare_amount']  # Target\n",
    "\n",
    "# # Identify categorical and numerical columns\n",
    "# categorical_features = ['Weather', 'Traffic Condition', 'Car Condition']\n",
    "# numerical_features = [col for col in X.columns if col not in categorical_features]\n",
    "\n",
    "# # Check for multicollinearity using VIF\n",
    "# def calculate_vif(df, features):\n",
    "#     vif_data = pd.DataFrame()\n",
    "#     vif_data[\"feature\"] = features\n",
    "#     vif_data[\"VIF\"] = [variance_inflation_factor(df[features].values, i) for i in range(len(features))]\n",
    "#     return vif_data\n",
    "\n",
    "# vif_data = calculate_vif(X, numerical_features)\n",
    "# print(vif_data)\n",
    "\n",
    "# # Remove features with VIF > 10\n",
    "# high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"feature\"].tolist()\n",
    "# X.drop(columns=high_vif_features, inplace=True)\n",
    "# numerical_features = [col for col in numerical_features if col not in high_vif_features]\n",
    "\n",
    "# # Preprocessing: MinMax Scaling for numerical & Ordinal Encoding for categorical\n",
    "# preprocessor = ColumnTransformer([\n",
    "#     ('num', MinMaxScaler(), numerical_features),\n",
    "#     ('cat', OrdinalEncoder(), categorical_features)\n",
    "# ])\n",
    "\n",
    "# # Split the dataset (80-20)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - MAE: 1.503137409082195, RMSE: 2.1209490676919125\n",
      "Linear Regression - Accuracy: 0.8392184267138022, F1 Score: 0.8452387232920073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------- LINEAR REGRESSION -----------------------\n",
    "\n",
    "# Pipeline for Linear Regression\n",
    "lr_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Cross-validation for Linear Regression\n",
    "lr_scores = cross_val_score(lr_pipeline, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Fit and Evaluate Linear Regression\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = mean_squared_error(y_test, y_pred_lr, squared=False)\n",
    "\n",
    "# Binarize predictions based on a threshold (e.g., median of y_test)\n",
    "threshold = y_test.median()\n",
    "y_pred_lr_class = (y_pred_lr >= threshold).astype(int)\n",
    "y_test_class = (y_test >= threshold).astype(int)\n",
    "\n",
    "# Accuracy and F1 Score\n",
    "accuracy_lr = accuracy_score(y_test_class, y_pred_lr_class)\n",
    "f1_lr = f1_score(y_test_class, y_pred_lr_class)\n",
    "\n",
    "print(f\"Linear Regression - MAE: {mae_lr}, RMSE: {rmse_lr}\")\n",
    "print(f\"Linear Regression - Accuracy: {accuracy_lr}, F1 Score: {f1_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "1 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\_utils.py\", line 72, in __call__\n",
      "    return self.func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py\", line 598, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py\", line 598, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\parallel.py\", line 136, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py\", line 192, in _parallel_build_trees\n",
      "    tree._fit(\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\tree\\_classes.py\", line 472, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "  File \"_tree.pyx\", line 172, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"_tree.pyx\", line 287, in sklearn.tree._tree.DepthFirstTreeBuilder.build\n",
      "  File \"_tree.pyx\", line 942, in sklearn.tree._tree.Tree._add_node\n",
      "  File \"_tree.pyx\", line 910, in sklearn.tree._tree.Tree._resize_c\n",
      "  File \"_utils.pyx\", line 35, in sklearn.tree._utils.safe_realloc\n",
      "MemoryError: could not allocate 16777216 bytes\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_forest.py\", line 489, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\utils\\parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py\", line 2007, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py\", line 1650, in _get_outputs\n",
      "    yield from self._retrieve()\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py\", line 1754, in _retrieve\n",
      "    self._raise_error_fast()\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py\", line 1789, in _raise_error_fast\n",
      "    error_job.get_result(self.timeout)\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py\", line 745, in get_result\n",
      "    return self._return_or_raise()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\joblib\\parallel.py\", line 763, in _return_or_raise\n",
      "    raise self._result\n",
      "MemoryError: could not allocate 16777216 bytes\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [-1.20837147 -1.46686213 -1.20418299         nan -1.2259822  -1.45961549\n",
      " -1.45996946 -1.20460629 -1.20246068 -1.45407071]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'model__max_depth': None, 'model__max_features': 'log2', 'model__min_samples_leaf': 3, 'model__min_samples_split': 5, 'model__n_estimators': 163}\n",
      "Random Forest (Optimized) - MAE: 1.1819829275781135, RMSE: 1.7629483210635255\n",
      "Best Random Forest Parameters: {'model__max_depth': None, 'model__max_features': 'log2', 'model__min_samples_leaf': 3, 'model__min_samples_split': 5, 'model__n_estimators': 163}\n",
      "Random Forest (Optimized) - MAE: 1.1819829275781135, RMSE: 1.7629483210635255\n",
      "Random Forest (Optimized) - Accuracy: 0.8767761472163987, F1 Score: 0.8802163112537794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# ---------------- FAST RANDOM FOREST (FIXED MEMORY) ----------------\n",
    "param_dist = {\n",
    "    'model__n_estimators': randint(100, 200),\n",
    "    'model__max_depth': [10, 20, None],\n",
    "    'model__min_samples_split': randint(2, 10),\n",
    "    'model__min_samples_leaf': randint(1, 4),\n",
    "    'model__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Pipeline for Random Forest\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Randomized Search for best parameters (reduced `n_iter=10` for speed)\n",
    "random_search = RandomizedSearchCV(\n",
    "    rf_pipeline, param_distributions=param_dist, \n",
    "    n_iter=10, cv=3, scoring='neg_mean_absolute_error',\n",
    "    n_jobs=-1, verbose=1, random_state=42\n",
    ")\n",
    "\n",
    "# Train Randomized Search (with smaller training set for tuning)\n",
    "subset_size = 250000  # Train on 250K rows instead of full 500K to save memory\n",
    "random_search.fit(X_train[:subset_size], y_train[:subset_size])\n",
    "\n",
    "# Best model after tuning\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Final Metrics\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)\n",
    "\n",
    "print(f\"Best Random Forest Parameters: {random_search.best_params_}\")\n",
    "print(f\"Random Forest (Optimized) - MAE: {mae_rf}, RMSE: {rmse_rf}\")\n",
    "\n",
    "# Binarize predictions based on a threshold (e.g., median of y_test)\n",
    "y_pred_rf_class = (y_pred_rf >= threshold).astype(int)\n",
    "\n",
    "# Accuracy and F1 Score\n",
    "accuracy_rf = accuracy_score(y_test_class, y_pred_rf_class)\n",
    "f1_rf = f1_score(y_test_class, y_pred_rf_class)\n",
    "\n",
    "print(f\"Best Random Forest Parameters: {random_search.best_params_}\")\n",
    "print(f\"Random Forest (Optimized) - MAE: {mae_rf}, RMSE: {rmse_rf}\")\n",
    "print(f\"Random Forest (Optimized) - Accuracy: {accuracy_rf}, F1 Score: {f1_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost - MAE: 1.141176266577775, RMSE: 1.7278117048109183\n",
      "XGBoost - Accuracy: 0.8792836491689617, F1 Score: 0.8822444396920445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Encode categorical features\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "xgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluation\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = mean_squared_error(y_test, y_pred_xgb, squared=False)\n",
    "\n",
    "print(f\"XGBoost - MAE: {mae_xgb}, RMSE: {rmse_xgb}\")\n",
    "\n",
    "# Binarize predictions based on a threshold (e.g., median of y_test)\n",
    "y_pred_xgb_class = (y_pred_xgb >= threshold).astype(int)\n",
    "\n",
    "# Accuracy and F1 Score\n",
    "accuracy_xgb = accuracy_score(y_test_class, y_pred_xgb_class)\n",
    "f1_xgb = f1_score(y_test_class, y_pred_xgb_class)\n",
    "\n",
    "print(f\"XGBoost - Accuracy: {accuracy_xgb}, F1 Score: {f1_xgb}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042930 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2908\n",
      "[LightGBM] [Info] Number of data points in the train set: 291920, number of used features: 20\n",
      "[LightGBM] [Info] Start training from score 8.603115\n",
      "LightGBM - MAE: 1.1491589450057116, RMSE: 1.737150668473005\n",
      "LightGBM - Accuracy: 0.8784067085953878, F1 Score: 0.8813001605136437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Encode categorical features\n",
    "X_train_encoded = preprocessor.fit_transform(X_train)\n",
    "X_test_encoded = preprocessor.transform(X_test)\n",
    "\n",
    "# LightGBM model\n",
    "lgb_model = lgb.LGBMRegressor(n_estimators=100, max_depth=6, learning_rate=0.1, random_state=42, n_jobs=-1)\n",
    "lgb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lgb = lgb_model.predict(X_test_encoded)\n",
    "\n",
    "# Evaluation\n",
    "mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n",
    "rmse_lgb = mean_squared_error(y_test, y_pred_lgb, squared=False)\n",
    "\n",
    "print(f\"LightGBM - MAE: {mae_lgb}, RMSE: {rmse_lgb}\")\n",
    "\n",
    "# Binarize predictions based on a threshold (e.g., median of y_test)\n",
    "y_pred_lgb_class = (y_pred_lgb >= threshold).astype(int)\n",
    "\n",
    "# Accuracy and F1 Score\n",
    "accuracy_lgb = accuracy_score(y_test_class, y_pred_lgb_class)\n",
    "f1_lgb = f1_score(y_test_class, y_pred_lgb_class)\n",
    "\n",
    "print(f\"LightGBM - Accuracy: {accuracy_lgb}, F1 Score: {f1_lgb}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
